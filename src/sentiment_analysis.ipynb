{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import mysql.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\HP Envy\\OneDrive - Swinburne University\\Data Science\\Innovation project\\craw_data\\Dat\\reddit_comments_HHSpolicy(2).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I'll start with a throat-clearing so I am not ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Appreciate your thorough reply mate, plenty of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Very sensible take. What about shipping? Ships...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TLDR; Twiggy has the capital and knowledge/exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I appreciate the TLDR at the top of the post. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Have a look at the work that Anglo American is...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The hype and emotive narrative they are hyping...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A bit like a cult leader drawing people in?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>From a technical perspective we can make green...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>It comes with massive government subsidies. So...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Yes IMO but it’s a matter of timing. Long term...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Long term it’s also a commodity, so it’s not g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>I am hopeful it works if only for the benefit ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I invested in the company as I like what I am ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Buy them for the fantastic steel/iron ore prod...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>They talk about the green side of the business...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>I was wondering that as well Unless the steel ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Twiggy is a business man and everything he is ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>I think its easier for him to foray into the a...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              comment\n",
       "0   I'll start with a throat-clearing so I am not ...\n",
       "1   Appreciate your thorough reply mate, plenty of...\n",
       "2   Very sensible take. What about shipping? Ships...\n",
       "3   TLDR; Twiggy has the capital and knowledge/exp...\n",
       "4   I appreciate the TLDR at the top of the post. ...\n",
       "5   Have a look at the work that Anglo American is...\n",
       "6   The hype and emotive narrative they are hyping...\n",
       "7         A bit like a cult leader drawing people in?\n",
       "8   From a technical perspective we can make green...\n",
       "9   It comes with massive government subsidies. So...\n",
       "10  Yes IMO but it’s a matter of timing. Long term...\n",
       "11  Long term it’s also a commodity, so it’s not g...\n",
       "12  I am hopeful it works if only for the benefit ...\n",
       "13  I invested in the company as I like what I am ...\n",
       "14  Buy them for the fantastic steel/iron ore prod...\n",
       "15  They talk about the green side of the business...\n",
       "16  I was wondering that as well Unless the steel ...\n",
       "17  Twiggy is a business man and everything he is ...\n",
       "18  I think its easier for him to foray into the a..."
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"feenix-mariadb.swin.edu.au\"\n",
    "user = \"s104489467\"\n",
    "password = \"101100\"\n",
    "database = \"s104489467_db\"\n",
    "connection = mysql.connector.connect(host=host,user=user,password=password,database=database)\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP Envy\\AppData\\Local\\Temp\\ipykernel_12488\\3614473737.py:2: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  df = pd.read_sql(sql_query, connection)\n"
     ]
    }
   ],
   "source": [
    "sql_query = \"select * from energy_data\"\n",
    "df = pd.read_sql(sql_query, connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>country</th>\n",
       "      <th>type</th>\n",
       "      <th>comment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>191</td>\n",
       "      <td>Australia</td>\n",
       "      <td>FMAA</td>\n",
       "      <td>https://12ft.io/https://www.smh.com.au/politic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>192</td>\n",
       "      <td>Australia</td>\n",
       "      <td>FMAA</td>\n",
       "      <td>The Coalition has attacked the production cred...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>193</td>\n",
       "      <td>Australia</td>\n",
       "      <td>FMAA</td>\n",
       "      <td>Production credits are better than handouts - ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>194</td>\n",
       "      <td>Australia</td>\n",
       "      <td>FMAA</td>\n",
       "      <td>Roll the dice with our labour rates, our envir...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>195</td>\n",
       "      <td>Australia</td>\n",
       "      <td>FMAA</td>\n",
       "      <td>Aiming for more economic complexity is good, b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2522</th>\n",
       "      <td>2713</td>\n",
       "      <td>Australia</td>\n",
       "      <td>windpower</td>\n",
       "      <td>Do you know where and what qualifications they...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2523</th>\n",
       "      <td>2714</td>\n",
       "      <td>Australia</td>\n",
       "      <td>windpower</td>\n",
       "      <td>In Australia you could try the big OEMs, which...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2524</th>\n",
       "      <td>2715</td>\n",
       "      <td>Australia</td>\n",
       "      <td>windpower</td>\n",
       "      <td>Awesome this is very helpful thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2525</th>\n",
       "      <td>2716</td>\n",
       "      <td>Australia</td>\n",
       "      <td>windpower</td>\n",
       "      <td>Search the group with the little magnifying gl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2526</th>\n",
       "      <td>2717</td>\n",
       "      <td>Australia</td>\n",
       "      <td>windpower</td>\n",
       "      <td>And quit saying the parks been running well</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2527 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        id    country       type  \\\n",
       "0      191  Australia       FMAA   \n",
       "1      192  Australia       FMAA   \n",
       "2      193  Australia       FMAA   \n",
       "3      194  Australia       FMAA   \n",
       "4      195  Australia       FMAA   \n",
       "...    ...        ...        ...   \n",
       "2522  2713  Australia  windpower   \n",
       "2523  2714  Australia  windpower   \n",
       "2524  2715  Australia  windpower   \n",
       "2525  2716  Australia  windpower   \n",
       "2526  2717  Australia  windpower   \n",
       "\n",
       "                                                comment  \n",
       "0     https://12ft.io/https://www.smh.com.au/politic...  \n",
       "1     The Coalition has attacked the production cred...  \n",
       "2     Production credits are better than handouts - ...  \n",
       "3     Roll the dice with our labour rates, our envir...  \n",
       "4     Aiming for more economic complexity is good, b...  \n",
       "...                                                 ...  \n",
       "2522  Do you know where and what qualifications they...  \n",
       "2523  In Australia you could try the big OEMs, which...  \n",
       "2524                Awesome this is very helpful thanks  \n",
       "2525  Search the group with the little magnifying gl...  \n",
       "2526        And quit saying the parks been running well  \n",
       "\n",
       "[2527 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\hp envy\\anaconda3\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\hp envy\\anaconda3\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp envy\\anaconda3\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp envy\\anaconda3\\lib\\site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp envy\\anaconda3\\lib\\site-packages (from nltk) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp envy\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: nltk>=3.8 in c:\\users\\hp envy\\anaconda3\\lib\\site-packages (from textblob) (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\hp envy\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\hp envy\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\hp envy\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\hp envy\\anaconda3\\lib\\site-packages (from nltk>=3.8->textblob) (4.65.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\hp envy\\appdata\\roaming\\python\\python311\\site-packages (from click->nltk>=3.8->textblob) (0.4.6)\n",
      "Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "   ---------------------------------------- 0.0/626.3 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/626.3 kB ? eta -:--:--\n",
      "   - ------------------------------------- 20.5/626.3 kB 330.3 kB/s eta 0:00:02\n",
      "   ----- --------------------------------- 81.9/626.3 kB 762.6 kB/s eta 0:00:01\n",
      "   -------------- ------------------------- 225.3/626.3 kB 1.5 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 368.6/626.3 kB 1.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 501.8/626.3 kB 2.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 583.7/626.3 kB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 626.3/626.3 kB 2.0 MB/s eta 0:00:00\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.18.0.post0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting vaderSentiment\n",
      "  Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl.metadata (572 bytes)\n",
      "Requirement already satisfied: requests in c:\\users\\hp envy\\anaconda3\\lib\\site-packages (from vaderSentiment) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\hp envy\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\hp envy\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\hp envy\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\hp envy\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2024.2.2)\n",
      "Downloading vaderSentiment-3.3.2-py2.py3-none-any.whl (125 kB)\n",
      "   ---------------------------------------- 0.0/126.0 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 10.2/126.0 kB ? eta -:--:--\n",
      "   --- ------------------------------------ 10.2/126.0 kB ? eta -:--:--\n",
      "   --------- ----------------------------- 30.7/126.0 kB 262.6 kB/s eta 0:00:01\n",
      "   --------------------------------- ---- 112.6/126.0 kB 656.4 kB/s eta 0:00:01\n",
      "   -------------------------------------- 126.0/126.0 kB 570.0 kB/s eta 0:00:00\n",
      "Installing collected packages: vaderSentiment\n",
      "Successfully installed vaderSentiment-3.3.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install vaderSentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting emojiNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading emoji-2.14.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Downloading emoji-2.14.0-py3-none-any.whl (586 kB)\n",
      "   ---------------------------------------- 0.0/586.9 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/586.9 kB ? eta -:--:--\n",
      "    --------------------------------------- 10.2/586.9 kB ? eta -:--:--\n",
      "   -- ------------------------------------ 30.7/586.9 kB 435.7 kB/s eta 0:00:02\n",
      "   ---- ---------------------------------- 71.7/586.9 kB 653.6 kB/s eta 0:00:01\n",
      "   ------------- -------------------------- 204.8/586.9 kB 1.4 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 368.6/586.9 kB 1.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 522.2/586.9 kB 2.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 586.9/586.9 kB 2.1 MB/s eta 0:00:00\n",
      "Installing collected packages: emoji\n",
      "Successfully installed emoji-2.14.0\n"
     ]
    }
   ],
   "source": [
    "pip install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\HP\n",
      "[nltk_data]     Envy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\HP\n",
      "[nltk_data]     Envy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\HP\n",
      "[nltk_data]     Envy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer= PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenization(text):\n",
    "    return nltk.word_tokenize(text)\n",
    "def to_lowecase(tokens):\n",
    "    return [word.lower() for word in tokens]\n",
    "def remove_stopwords(tokens):\n",
    "    return [word for word in tokens if word not in stop_words]\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(word) for word in tokens]\n",
    "def lemmatize_tokens(tokens):\n",
    "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
    "def remove_special_characters(tokens):\n",
    "    return [re.sub(r'[^\\w\\s]', '', token) for token in tokens if re.sub(r'[^\\w\\s]', '', token)]\n",
    "def handle_emojis(tokens):\n",
    "    return [emoji.demojize(token) for token in tokens]\n",
    "def correct_spelling(tokens):\n",
    "    corrected_tokens = []\n",
    "    for token in tokens:\n",
    "        corrected_token = str(TextBlob(token).correct())\n",
    "        corrected_tokens.append(corrected_token)\n",
    "    return corrected_tokens\n",
    "def preprocess_text(text):\n",
    "    tokens = tokenization(text)\n",
    "    tokens = to_lowecase(tokens)\n",
    "    tokens = remove_stopwords(tokens)\n",
    "    tokens = stem_tokens(tokens)\n",
    "    tokens = lemmatize_tokens(tokens)\n",
    "    tokens = remove_special_characters(tokens)\n",
    "    tokens = handle_emojis(tokens)\n",
    "    tokens = correct_spelling(tokens)\n",
    "    return ' '.join(tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_comment'] = df['comment'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv(\"output.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\HP\n",
      "[nltk_data]     Envy\\AppData\\Roaming\\nltk_data...\n"
     ]
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')\n",
    "analyzer = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_sentiment(text):\n",
    "    if not text:\n",
    "        return {'compound': 0.0, 'neg': 0.0, 'neu': 0.0, 'pos': 0.0}\n",
    "    return analyzer.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['cleaned_comment'].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP Envy\\AppData\\Local\\Temp\\ipykernel_13276\\1254702149.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['sentiment'] = df['cleaned_comment'].apply(analyze_sentiment)\n"
     ]
    }
   ],
   "source": [
    "df['sentiment'] = df['cleaned_comment'].apply(analyze_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP Envy\\AppData\\Local\\Temp\\ipykernel_13276\\1043370931.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['compound'] = df['sentiment'].apply(lambda x: x['compound'])\n"
     ]
    }
   ],
   "source": [
    "df['compound'] = df['sentiment'].apply(lambda x: x['compound'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_score(compound_score):\n",
    "    # Rescale from [-1, 1] to [1, 5]\n",
    "    return 1 + (compound_score + 1) * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP Envy\\AppData\\Local\\Temp\\ipykernel_13276\\6971240.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df['rescaled_compound'] = df['compound'].apply(rescale_score)\n"
     ]
    }
   ],
   "source": [
    "df['rescaled_compound'] = df['compound'].apply(rescale_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2515 entries, 0 to 2526\n",
      "Data columns (total 8 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Unnamed: 0       2515 non-null   int64  \n",
      " 1   id               2515 non-null   int64  \n",
      " 2   country          2515 non-null   object \n",
      " 3   type             2515 non-null   object \n",
      " 4   comment          2515 non-null   object \n",
      " 5   cleaned_comment  2515 non-null   object \n",
      " 6   sentiment        2515 non-null   object \n",
      " 7   compound         2515 non-null   float64\n",
      "dtypes: float64(1), int64(2), object(5)\n",
      "memory usage: 176.8+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2515.000000\n",
       "mean        3.433355\n",
       "std         0.986160\n",
       "min         1.052600\n",
       "25%         3.000000\n",
       "50%         3.507400\n",
       "75%         4.273800\n",
       "max         4.992800\n",
       "Name: rescaled_compound, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['rescaled_compound'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"output_sentiment.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
